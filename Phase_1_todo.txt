## Gap List Reframed With Impact / Effort

SUMMARY OF WHAT’S CLEARLY “NOT DONE” FOR PHASE 1
Streaming Gemini responses (Spec #1)
Function calling / structured output mode (Spec #1)
UI platform per spec (Streamlit/Gradio) – divergence
Configurable high row counts (scaling to ~1000 per table)
Constraint-respecting output (we stopped repairing)
Per-table modification workflow in UI
Dataset history browsing UI
CSV/ZIP download button in UI
Langfuse observability integration
Dockerfile and docker-compose setup
Row count parameter (explicit) separate from prompt hints

1. Streaming Gemini responses  
   Current: Per-table synchronous fetch (only simulated chunking in socket path).  
   Minimal Win: Use existing socket pathway to stream parsed rows as they arrive (you already have baseline deterministic streaming code).  
   Effort: Medium (reuse generator loop → emit tokens or partial JSON slices).  
   Risk: Low.

2. Function calling / structured output  
   Current: Heuristic JSON salvage; no schema-constrained output.  
   Minimal Win: Wrap prompt with: “Respond ONLY with valid JSON array of objects.” Already doing, but upgrade to explicit model-config structured output (if Gemini SDK supports).  
   Effort: Medium (depends on SDK support).  
   Risk: If not supported in your current model variant, fallback gracefully.

3. UI platform divergence (Streamlit/Gradio)  
   You deliberately chose React + Node.  
   Action: Document this decision clearly in README so the spec deviation is intentional.  
   Effort: Low.

4. Configurable high row counts (≈ up to 1000 / table)  
   Current: Advisory only; still using clamp logic (10 previously).  
   Action: Add explicit numeric input (row target) + pass as hint, do NOT enforce values (per your philosophy), but warn if > token budget.  
   Effort: Low.

5. Constraint‑respecting output (you disabled repair)  
   Spec requires integrity; you want raw.  
   Action: Provide toggle “Integrity Repair (optional)” default OFF. Run repair + validation second pass if toggled.  
   Effort: Low–Medium (repair code exists; just re-gate).  
   Risk: None if clearly optional.

6. Per-table modification workflow UI  
   Current: Endpoint exists.  
   Action: Add UI: table selector → prompt box → “Apply” → diff & revalidation display.  
   Effort: Medium.

7. Dataset history browsing UI  
   Current: Endpoint present (`/api/datasets`).  
   Action: Add side panel or tab to list (id, name, created_at, row counts), clickable to load preview.  
   Effort: Low.

8. CSV/ZIP download button  
   Current: Endpoint exists (`/api/datasets/:id/export`).  
   Action: Add button in preview header → triggers file download.  
   Effort: Low.

9. Langfuse observability  
   Current: `initializeLangfuse()` placeholder.  
   Action: Add spans: parse_schema, ai_generate_table, validation, modify_dataset. Capture prompt & token metadata (redact DDL if sensitive flag).  
   Effort: Medium.

10. Docker & docker-compose  
    Current: None.  
    Action: Add `Dockerfile.server`, `Dockerfile.client`, `docker-compose.yml` (Postgres + server + client), environment examples.  
    Effort: Medium.

11. Explicit row count parameter (separate from prompt)  
    Overlaps with item 4.  
    Action: Add “Target Rows (advisory)” numeric input; inserted into prompt scaffolding before free-form instructions.  
    Effort: Low.

12. Advanced column-level control (null %, date formats)  
    Minimal Viable: Accept JSON config block area (optional) like:  
    ```
    {
      "nullProbability": { "Books": { "title": 0.0 }, "Authors": { "bio": 0.3 } },
      "dateFormat": "ISO"
    }
    ```  
    Pass hints into prompt only (still non-enforcing).  
    Effort: Medium (UI + injection).  
    Enforcement (hard constraints) can stay deferred.

13. Retry / robust generation strategy  
    Current: Single attempt per table.  
    Action: Add simple retry: if parse fails or returns empty → 1 retry with reduced temperature before fallback deterministic. Log attempts.  
    Effort: Low.

14. Large schema performance validation (scaling)  
    Current: Not tested.  
    Action: Add a lightweight benchmark script (e.g., generate synthetic in loop with stub schema for 5–7 tables × 100/500/1000 advisory rows) measuring latency & memory; log to console. Not part of UX—dev-only.  
    Effort: Medium.

## Recommended Implementation Order (Max Value Fast)

Priority 1 (User-visible completeness):  
- (6) Per-table modification UI  
- (7) Dataset history UI  
- (8) Download button  
- (11/4) Advisory Target Rows input  
- (13) Retry logic (stability)

Priority 2 (Spec alignment & optional quality):  
- (5) Integrity repair toggle (off by default)  
- (1) Streaming real AI partials (if feasible quickly)  
- (2) Structured output mode improvement  
- (9) Langfuse traces  
- (10) Docker packaging

Priority 3 (Advanced / nice-to-have):  
- (12) Column-level controls (hints only)  
- (14) Benchmark script  
- README doc for divergence (3)

## Concrete Task Breakdown (First Execution Batch Proposal)

Batch A (UI + small server changes):
1. Add `/client/src/components/DatasetList` to show datasets.
2. Add download button into `DataPreview`.
3. Add modification panel below table (prompt + apply).
4. Add “Target Rows” numeric input near temperature.
5. Pass `config.numRecords` (advisory) into generation request (already partially supported).

Batch B (Server robustness):
6. Implement simple retry in `dataGenerator.generateSyntheticData` (one second attempt with temp *= 0.7 if parse empty).
7. Add optional `config.integrityRepair` flag (default false) to call existing `autoRepairForeignKeys`.

Batch C (DevOps / observability placeholders):
8. Add minimal Langfuse trace wrapper (guarded by env var).
9. Add Dockerfile + docker-compose skeleton (if you confirm now).

Let me know which batch (A only, A+B, or full A+B+C) you want me to start implementing right now. If you pick one, I’ll proceed immediately with code changes.

Just reply with one of:
- “Do A”
- “Do A+B”
- “Do all (A+B+C)”
- Or customize the list.

Your call—what’s the next move?    Pass hints into prompt only (still non-enforcing).  
    Effort: Medium (UI + injection).  
    Enforcement (hard constraints) can stay deferred.

13. Retry / robust generation strategy  
    Current: Single attempt per table.  
    Action: Add simple retry: if parse fails or returns empty → 1 retry with reduced temperature before fallback deterministic. Log attempts.  
    Effort: Low.

14. Large schema performance validation (scaling)  
    Current: Not tested.  
    Action: Add a lightweight benchmark script (e.g., generate synthetic in loop with stub schema for 5–7 tables × 100/500/1000 advisory rows) measuring latency & memory; log to console. Not part of UX—dev-only.  
    Effort: Medium.

## Recommended Implementation Order (Max Value Fast)

Priority 1 (User-visible completeness):  
- (6) Per-table modification UI  
- (7) Dataset history UI  
- (8) Download button  
- (11/4) Advisory Target Rows input  
- (13) Retry logic (stability)

Priority 2 (Spec alignment & optional quality):  
- (5) Integrity repair toggle (off by default)  
- (1) Streaming real AI partials (if feasible quickly)  
- (2) Structured output mode improvement  
- (9) Langfuse traces  
- (10) Docker packaging

Priority 3 (Advanced / nice-to-have):  
- (12) Column-level controls (hints only)  
- (14) Benchmark script  
- README doc for divergence (3)

## Concrete Task Breakdown (First Execution Batch Proposal)

Batch A (UI + small server changes):
1. Add `/client/src/components/DatasetList` to show datasets.
2. Add download button into `DataPreview`.
3. Add modification panel below table (prompt + apply).
4. Add “Target Rows” numeric input near temperature.
5. Pass `config.numRecords` (advisory) into generation request (already partially supported).

Batch B (Server robustness):
6. Implement simple retry in `dataGenerator.generateSyntheticData` (one second attempt with temp *= 0.7 if parse empty).
7. Add optional `config.integrityRepair` flag (default false) to call existing `autoRepairForeignKeys`.

Batch C (DevOps / observability placeholders):
8. Add minimal Langfuse trace wrapper (guarded by env var).
9. Add Dockerfile + docker-compose skeleton (if you confirm now).

Here’s a concise Phase 1 status table (Done vs. Partial vs. Not Started), with quick notes and next steps.

| Feature / Deliverable | Status | Notes / Gap | Next Recommended Step |
|-----------------------|--------|-------------|------------------------|
| Schema upload & parsing | Done | Schema ingested, primary/foreign keys recognized | None |
| Per-table AI generation (Gemini) | Done | One call per table; raw JSON cleaned | Consider structured output later |
| Deterministic fallback (empty AI table) | Done | Only used when AI returns nothing | Optional: expose toggle to disable |
| Validation (PK/FK/NOT NULL reporting) | Done | Purely informational—no mutation | Add UI severity badges |
| Strict raw AI mode (no normalization/repair) | Done | Now default; preserves AI output | Optional toggle to re-enable repair path |
| Max tokens support (backend + UI) | Done | Clamped + shown as maxTokensApplied | Add tooltip explaining truncation |
| Temperature control | Done | Passed through to model; surfaced | Possibly persist last-used setting |
| Unified Raw / Meta / Validation viewer | Done | Consolidated into one component | Add download for each pane |
| Advisory row hints via prompt parsing | Done | Non-enforcing; stored in meta.rowHints | Add explicit numeric inputs (see below) |
| Always persist dataset (removed toggle) | Done | Persistence implicit | Add retention/cleanup policy |
| Foreign key / PK repair module (code) | Partial | Logic exists but bypassed in strict mode | Hook behind optional “Integrity Repair” toggle |
| Modification workflow (regenerate per table) | Partial | Backend capability present; UI absent | Add per-table prompt + regenerate button |
| Dataset listing (backend) | Partial | Endpoint exists (assumed); no UI list | Build sidebar/history panel |
| Export endpoint (server) | Partial | Route present; no UI button | Add “Download (CSV/ZIP)” actions |
| Streaming (true incremental model output) | Not Started (Sim only) | Only simulated via socket; not model stream | Investigate Gemini streaming API |
| Structured output / function calling | Not Started | Raw JSON heuristic clean only | Adopt schema-constrained generation |
| Retry / fallback strategy (AI errors) | Not Started | Single attempt only | Add exponential backoff + deterministic fallback log |
| Dataset history UI | Not Started | No timeline/version view | Add paginated list with timestamps |
| Download/export UI | Not Started | Needs buttons & format selector | Implement CSV/JSON/ZIP bundle |
| Explicit numeric row count inputs | Not Started | Only advisory hints parsed from free text | Add per-table + global row count fields (non-enforcing at first) |
| Integrity repair toggle (UI) | Not Started | Backend path dormant | Add switch: Raw vs. Repair |
| Langfuse / observability instrumentation | Not Started (stub) | Placeholder only | Wrap generation calls, log latency, token usage |
| High row-count performance (e.g. 1K+) | Not Started | No load/perf profiling | Add perf test & batching strategy |
| Column-level advanced controls (null %, formats) | Not Started | Free-form prompt only | Add optional schema augmentation UI |
| Dockerfile + docker-compose | Not Started | Local-only deployment so far | Add multi-service containerization |
| README Phase 1 alignment & philosophy note | Not Started | README likely outdated vs. raw mode | Document raw vs. repair modes |
| Automated tests (unit/integration) | Not Started | No test harness mentioned | Add minimal Jest (backend) + component tests |
| Error surfaced to UI (generation failures) | Partial | Likely basic error toast/log only | Standardize error object + retry affordance |
| Security / input sanitization | Not Started | Prompt content passed through directly | Sanitize & add size limits |
| Configurable retention / cleanup | Not Started | Datasets accumulate | Add TTL or manual prune action |
| CI workflow (lint/build/test) | Not Started | Manual validation only | Add GitHub Actions pipeline |

Legend:  
Done = Feature complete and stable.  
Partial = Some functionality exists but missing UI, polish, or toggle.  
Not Started = No meaningful implementation yet.

Priority suggestions (fastest impact first):
1. Per-table modification UI + regenerate
2. Dataset history + export/download UI
3. Integrity repair toggle (off by default)
4. Retry logic + surfaced errors
5. Structured output or stronger JSON schema enforcement
6. Dockerization + README update
7. Observability (Langfuse) + performance profiling

/// >>>>>>>>>>>>>>>> nexc iteration : 
Implementation Order Recap

(1) Retry
Adds resilience: a single low‑temperature retry on malformed or empty AI output reduces silent fallbacks and improves dataset quality without user intervention.

(2) Repair Toggle
Lets users opt into constraint fixes (FK/PK alignment) while keeping default raw mode; satisfies spec requirement for integrity without forcing post‑processing.

(3) Structured Output
Reduces parsing fragility by enforcing or validating strict JSON arrays of objects; lowers downstream error handling and increases reliability of meta/validation.

(4) Row Hints UI
Exposes advisory target row counts (global + optional per-table) so users can steer dataset scale explicitly instead of burying hints inside free‑form prompt text.

(5) Langfuse
Provides observability (timings, retry flags, integrity toggle usage, row counts) enabling performance tuning, debugging, and future quality benchmarks.

(6) Docker/Compose
Ensures reproducible, one‑command environment (app + Postgres) for teammates and deployment; reduces “works on my machine” drift.

(7) README
Aligns expectations: documents decisions (no streaming in Phase 1), usage instructions, environment variables, and roadmap—critical for handoff and review.

(8) Optional Metadata Panel
Improves transparency by surfacing prompt hash, retries, integrity status, row hints, and token info—helps users trust and audit generation outcomes.
